{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YAwdPOYaHPd"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, SGDClassifier, LogisticRegression\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, roc_auc_score, precision_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.model_selection import cross_validate\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"my_module\")\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pylab as plt\n",
        "from scipy import interp\n",
        "from sklearn.linear_model import LogisticRegression, Lasso, RidgeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "datasets = [(X_0_5s_c, y_0_5s_c), (X_1s_c, y_1s_c), (X_2s_c, y_2s_c), (X_3s_c, y_3s_c), (X_4s_c, y_4s_c), (X_5s_c, y_5s_c)]\n",
        "labels = [0.5, 1, 2, 3, 4, 5]\n",
        "random_state = np.random.RandomState(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "fig1, ax = plt.subplots(ncols=3, figsize=[32, 24], nrows=2, sharex=True, sharey=True)\n",
        "\n",
        "class_combinations = [(0, [1, 2]), (1, [0, 2]), (2, [0, 1])]\n",
        "class_labels = ['Baseline vs (CTA, CTA+RTA)', 'CTA vs (Baseline, CTA+RTA)', 'CTA+RTA vs (Baseline, CTA)']\n",
        "\n",
        "for i, (x, y) in enumerate(datasets):\n",
        "    print('Processing', i)\n",
        "    classifiers = [\n",
        "        ('Logistic Regression', LogisticRegression(random_state=random_state)),\n",
        "        ('MLP Classifier', MLPClassifier(random_state=random_state)),\n",
        "        ('Linear Discriminant Analysis', LinearDiscriminantAnalysis()),\n",
        "        ('KNN', KNeighborsClassifier()),\n",
        "        ('Decision Tree', DecisionTreeClassifier(random_state=random_state)),\n",
        "        ('Random Forest', RandomForestClassifier(random_state=random_state)),\n",
        "        ('AdaBoost', AdaBoostClassifier(random_state=random_state)),\n",
        "        ('XGBoost', xgb.XGBClassifier(random_state=random_state)),\n",
        "        ('LightGBM', lgb.LGBMClassifier(random_state=random_state))\n",
        "    ]\n",
        "\n",
        "    mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "    mean_tprs_micro = []\n",
        "    mean_aucs_micro = []\n",
        "\n",
        "    for name, clf in classifiers:\n",
        "        cv = KFold(n_splits=5, shuffle=True)\n",
        "        tprs_micro = []\n",
        "        aucs_micro = []\n",
        "\n",
        "        for idx, (class_idx, rest_classes) in enumerate(class_combinations):\n",
        "            for train, test in cv.split(x, y):\n",
        "                clf.fit(x.iloc[train], y.iloc[train])\n",
        "                prediction = clf.predict_proba(x.iloc[test])\n",
        "\n",
        "                lb = LabelBinarizer()\n",
        "                lb.fit(y)\n",
        "                y_test_bin = lb.transform(y.iloc[test])\n",
        "                y_test_bin_one_vs_rest = np.hstack((y_test_bin[:, class_idx].reshape(-1, 1), y_test_bin[:, rest_classes]))\n",
        "\n",
        "                fpr_micro, tpr_micro, _ = roc_curve(y_test_bin.ravel(), prediction.ravel())\n",
        "                tprs_micro.append(interp(mean_fpr, fpr_micro, tpr_micro))\n",
        "                roc_auc_micro = auc(fpr_micro, tpr_micro)\n",
        "                aucs_micro.append(roc_auc_micro)\n",
        "\n",
        "        mean_tpr_micro = np.mean(tprs_micro, axis=0)\n",
        "        mean_auc_micro = np.mean(aucs_micro)\n",
        "        mean_tprs_micro.append(mean_tpr_micro)\n",
        "        mean_aucs_micro.append(mean_auc_micro)\n",
        "\n",
        "        ax.flat[i].plot(mean_fpr, mean_tpr_micro, label='%s(AUC = %0.3f)' % (name, mean_auc_micro), lw=2)\n",
        "\n",
        "    ax.flat[i].plot([0, 1], [0, 1], linestyle='--', lw=2, color='black')\n",
        "    ax.flat[i].set_xlabel('False Positive Rate', fontweight='bold', fontsize='13')\n",
        "    ax.flat[i].set_ylabel('True Positive Rate', fontweight='bold', fontsize='13')\n",
        "    ax.flat[i].set_title('ROC curve in ' + str(labels[i]) + 's window', fontweight='bold', fontsize='13')\n",
        "    ax.flat[i].legend(loc='lower right', prop={'weight': 'bold', 'size': '13'})\n",
        "plt.savefig('roc_curves.png', dpi=300, bbox_inches='tight')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WhsTi9yDadx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pylab as plt\n",
        "from scipy import interp\n",
        "from sklearn.linear_model import LogisticRegression, Lasso, RidgeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "datasets = [(X_0_5s_d, y_0_5s_d), (X_1s_d, y_1s_d), (X_2s_d, y_2s_d), (X_3s_d, y_3s_d), (X_4s_d, y_4s_d), (X_5s_d, y_5s_d)]\n",
        "labels = [0.5, 1, 2, 3, 4, 5]\n",
        "random_state = np.random.RandomState(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "fig1, ax = plt.subplots(ncols=3, figsize=[32, 24], nrows=2, sharex=True, sharey=True)\n",
        "\n",
        "class_combinations = [(0, [1, 2]), (1, [0, 2]), (2, [0, 1])]\n",
        "class_labels = ['Baseline vs (Attention, Distraction)', 'Distraction vs (Baseline, Attention)', 'Attention vs (Baseline, Distraction)']\n",
        "\n",
        "for i, (x, y) in enumerate(datasets):\n",
        "    print('Processing', i)\n",
        "    classifiers = [\n",
        "        ('Logistic Regression', LogisticRegression(random_state=random_state)),\n",
        "        ('MLP Classifier', MLPClassifier(random_state=random_state)),\n",
        "        ('Linear Discriminant Analysis', LinearDiscriminantAnalysis()),\n",
        "        ('KNN', KNeighborsClassifier()),\n",
        "        ('Decision Tree', DecisionTreeClassifier(random_state=random_state)),\n",
        "        ('Random Forest', RandomForestClassifier(random_state=random_state)),\n",
        "        ('AdaBoost', AdaBoostClassifier(random_state=random_state)),\n",
        "        ('XGBoost', xgb.XGBClassifier(random_state=random_state)),\n",
        "        ('LightGBM', lgb.LGBMClassifier(random_state=random_state))\n",
        "    ]\n",
        "\n",
        "    mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "    mean_tprs_micro = []\n",
        "    mean_aucs_micro = []\n",
        "\n",
        "    for name, clf in classifiers:\n",
        "        cv = KFold(n_splits=5, shuffle=True)\n",
        "        tprs_micro = []\n",
        "        aucs_micro = []\n",
        "\n",
        "        for idx, (class_idx, rest_classes) in enumerate(class_combinations):\n",
        "            for train, test in cv.split(x, y):\n",
        "                clf.fit(x.iloc[train], y.iloc[train])\n",
        "                prediction = clf.predict_proba(x.iloc[test])\n",
        "\n",
        "                lb = LabelBinarizer()\n",
        "                lb.fit(y)\n",
        "                y_test_bin = lb.transform(y.iloc[test])\n",
        "                y_test_bin_one_vs_rest = np.hstack((y_test_bin[:, class_idx].reshape(-1, 1), y_test_bin[:, rest_classes]))\n",
        "\n",
        "                fpr_micro, tpr_micro, _ = roc_curve(y_test_bin.ravel(), prediction.ravel())\n",
        "                tprs_micro.append(interp(mean_fpr, fpr_micro, tpr_micro))\n",
        "                roc_auc_micro = auc(fpr_micro, tpr_micro)\n",
        "                aucs_micro.append(roc_auc_micro)\n",
        "\n",
        "        mean_tpr_micro = np.mean(tprs_micro, axis=0)\n",
        "        mean_auc_micro = np.mean(aucs_micro)\n",
        "        mean_tprs_micro.append(mean_tpr_micro)\n",
        "        mean_aucs_micro.append(mean_auc_micro)\n",
        "\n",
        "        ax.flat[i].plot(mean_fpr, mean_tpr_micro, label='%s(AUC = %0.3f)' % (name, mean_auc_micro), lw=2)\n",
        "\n",
        "    ax.flat[i].plot([0, 1], [0, 1], linestyle='--', lw=2, color='black')\n",
        "    ax.flat[i].set_xlabel('False Positive Rate', fontweight='bold', fontsize='13')\n",
        "    ax.flat[i].set_ylabel('True Positive Rate', fontweight='bold', fontsize='13')\n",
        "    ax.flat[i].set_title('ROC curve in ' + str(labels[i]) + 's window', fontweight='bold', fontsize='13')\n",
        "    ax.flat[i].legend(loc='lower right', prop={'weight': 'bold', 'size': '13'})\n",
        "plt.savefig('roc_curves.png', dpi=300, bbox_inches='tight')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0LbRFde_cSOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "def benchmark_models_3(X, y):\n",
        "    models = {\n",
        "        'Logistic Regression': LogisticRegression(multi_class='auto'),\n",
        "        'Ridge Classifier': RidgeClassifier(),\n",
        "        'SGD Classifier': SGDClassifier(),\n",
        "        'Support Vector Classifier': SVC(),\n",
        "        'Decision Tree': DecisionTreeClassifier(),\n",
        "        'Random Forest': RandomForestClassifier(),\n",
        "        'AdaBoost': AdaBoostClassifier(),\n",
        "        'MLP Classifier': MLPClassifier(),\n",
        "        'Linear Discriminant Analysis': LinearDiscriminantAnalysis(),\n",
        "        'KNN': KNeighborsClassifier(),\n",
        "        'XGBoost': xgb.XGBClassifier(objective='multi:softmax'),\n",
        "        'LightGBM': lgb.LGBMClassifier()\n",
        "    }\n",
        "\n",
        "    cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
        "\n",
        "    scoring = {'accuracy': 'accuracy',\n",
        "               'f1_score_micro': 'f1_micro',\n",
        "               'f1_score_macro': 'f1_macro',\n",
        "               'recall_micro': 'recall_micro',\n",
        "               'recall_macro': 'recall_macro',\n",
        "               'precision_micro': 'precision_micro',\n",
        "               'precision_macro': 'precision_macro'}\n",
        "\n",
        "    results = pd.DataFrame(columns=['Accuracy', 'F1 Score ', 'Recall ', 'Precision'])\n",
        "\n",
        "    for name, model in models.items():\n",
        "        if isinstance(model, (LogisticRegression, RidgeClassifier, SGDClassifier, SVC)):\n",
        "            # Use the \"ovr\" strategy for binary relevance\n",
        "            model = OneVsRestClassifier(model)\n",
        "        elif isinstance(model, (lgb.LGBMClassifier)):\n",
        "            # Set the \"objective\" parameter to \"multiclass\" for multi-class classification\n",
        "            model = model.set_params(objective='multiclass')\n",
        "\n",
        "        scores = cross_validate(model, X, y, cv=cv, scoring=scoring)\n",
        "        results.loc[name] = [\n",
        "            scores['test_accuracy'].mean(),\n",
        "            #scores['test_f1_score_micro'].mean(),\n",
        "            scores['test_f1_score_macro'].mean(),\n",
        "            #scores['test_recall_micro'].mean(),\n",
        "            scores['test_recall_macro'].mean(),\n",
        "            #scores['test_precision_micro'].mean(),\n",
        "            scores['test_precision_macro'].mean()\n",
        "        ]\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "iRhqcHUDcYZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark_models_3(X_1s_d, y_1s_d)"
      ],
      "metadata": {
        "id": "944jq-Wbkj8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark_models_3(X_1s_c, y_1s_c)"
      ],
      "metadata": {
        "id": "9LQkoywwk3Me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "items = (['Temporal & Movement'] * 5) + (['Gaze & Pupil'] * 45) + (['Head Movement & Orientation'] * 17)"
      ],
      "metadata": {
        "id": "1XZPMRvft8r_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Distraction"
      ],
      "metadata": {
        "id": "_JYuIFCm-eb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import eli5\n",
        "from eli5.sklearn import PermutationImportance\n",
        "from sklearn.model_selection import KFold\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "RANDOM_STATE = 1\n",
        "kf = KFold(n_splits=5)\n",
        "clf = lgb.LGBMClassifier()\n",
        "X, y = X_1s_d, y_1s_d\n",
        "perm_importances = []\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "    clf.fit(X_train, y_train, eval_set=[(X_test, y_test)])\n",
        "    perm = PermutationImportance(clf, random_state=RANDOM_STATE).fit(X_test, y_test)\n",
        "    perm_importances.append(perm)\n",
        "\n",
        "# Calculate the importance scores for each fold\n",
        "importance_scores = np.array([perm.feature_importances_ for perm in perm_importances])\n",
        "\n",
        "# Calculate summary statistics for each feature\n",
        "min_scores = np.min(importance_scores, axis=0)\n",
        "q1_scores = np.percentile(importance_scores, 25, axis=0)\n",
        "mean_scores = np.mean(importance_scores, axis=0)\n",
        "median_scores = np.median(importance_scores, axis=0)\n",
        "q3_scores = np.percentile(importance_scores, 75, axis=0)\n",
        "max_scores = np.max(importance_scores, axis=0)\n",
        "\n",
        "# Create a DataFrame with feature names as index\n",
        "summary_df = pd.DataFrame({\n",
        "    'Min': min_scores,\n",
        "    'Q1': q1_scores,\n",
        "    'Mean': mean_scores,\n",
        "    'Median': median_scores,\n",
        "    'Q3': q3_scores,\n",
        "    'Max': max_scores\n",
        "}, index=X.columns)\n",
        "\n",
        "# Print the summary DataFrame\n",
        "print(summary_df)\n"
      ],
      "metadata": {
        "id": "TblhL-G0t3cE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_df['group'] = items\n",
        "summary_df = summary_df.reset_index()\n",
        "summary_df = summary_df.rename(columns={'index': 'label'})"
      ],
      "metadata": {
        "id": "exFlqN7R_WkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_grouped_df = summary_df.groupby('group', group_keys=False).apply(lambda group: group.sort_values(by='Median', ascending=False))"
      ],
      "metadata": {
        "id": "R7Y5LDCQ_naK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df = sorted_grouped_df.groupby('group').head(5)\n",
        "filtered_df = filtered_df.reset_index(drop = True)\n",
        "filtered_df_2 = filtered_df\n",
        "filtered_df_2['Median'] = filtered_df_2['Median'] * 100\n",
        "filtered_df_2['Q1'] = filtered_df_2['Q1'] * 100\n",
        "filtered_df_2['Q3'] = filtered_df_2['Q3'] * 100"
      ],
      "metadata": {
        "id": "TWjoCsPH_qHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install forestplot\n",
        "import forestplot as fp"
      ],
      "metadata": {
        "id": "dBg62Ddq_tl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp.forestplot(filtered_df_2,  # the dataframe with results data\n",
        "              estimate='Median',  # col containing estimated effect size\n",
        "              ll='Q1', hl='Q3',  # columns containing conf. int. lower and higher limits\n",
        "              varlabel='label',  # column containing variable label\n",
        "              capitalize=\"capitalize\",  # Capitalize labels\n",
        "              groupvar=\"group\",  # Add variable groupings\n",
        "              #decimal_precision=4,\n",
        "              group_order=['Temporal & Movement', 'Gaze & Pupil', 'Head Movement & Orientation'],\n",
        "              #ylabel=\"Confidence interval\",  # y-label title\n",
        "              xlabel=\"Decrease in Accuracy Score % \",  # x-label title\n",
        "              **{#\"marker\": \"D\",  # set maker symbol as diamond\n",
        "                 #\"markersize\": 35,  # adjust marker size\n",
        "                 \"xlinestyle\": (0, (10, 5)),  # long dash for x-reference line\n",
        "                 \"xlinecolor\": \"#808080\",  # gray color for x-reference line\n",
        "                 \"xtick_size\": 12,  # adjust x-ticker fontsize\n",
        "                }\n",
        "              )"
      ],
      "metadata": {
        "id": "qiZtQHCu_xHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####CTA/RTA"
      ],
      "metadata": {
        "id": "EZh-zKRs-h-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import eli5\n",
        "from eli5.sklearn import PermutationImportance\n",
        "from sklearn.model_selection import KFold\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "RANDOM_STATE = 1\n",
        "kf = KFold(n_splits=5)\n",
        "clf = lgb.LGBMClassifier()\n",
        "X, y = X_1s_c, y_1s_c\n",
        "perm_importances = []\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "    clf.fit(X_train, y_train, eval_set=[(X_test, y_test)])\n",
        "    perm = PermutationImportance(clf, random_state=RANDOM_STATE).fit(X_test, y_test)\n",
        "    perm_importances.append(perm)\n",
        "\n",
        "importance_scores = np.array([perm.feature_importances_ for perm in perm_importances])\n",
        "\n",
        "min_scores = np.min(importance_scores, axis=0)\n",
        "q1_scores = np.percentile(importance_scores, 25, axis=0)\n",
        "mean_scores = np.mean(importance_scores, axis=0)\n",
        "median_scores = np.median(importance_scores, axis=0)\n",
        "q3_scores = np.percentile(importance_scores, 75, axis=0)\n",
        "max_scores = np.max(importance_scores, axis=0)\n",
        "\n",
        "summary_df = pd.DataFrame({\n",
        "    'Min': min_scores,\n",
        "    'Q1': q1_scores,\n",
        "    'Mean': mean_scores,\n",
        "    'Median': median_scores,\n",
        "    'Q3': q3_scores,\n",
        "    'Max': max_scores\n",
        "}, index=X.columns)\n"
      ],
      "metadata": {
        "id": "DrAXDjhzt4gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_df['group'] = items\n",
        "summary_df = summary_df.reset_index()\n",
        "summary_df = summary_df.rename(columns={'index': 'label'})\n"
      ],
      "metadata": {
        "id": "mCGaJb6D9ZiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_grouped_df = summary_df.groupby('group', group_keys=False).apply(lambda group: group.sort_values(by='Median', ascending=False))"
      ],
      "metadata": {
        "id": "FDsC_b0o-7CS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df = sorted_grouped_df.groupby('group').head(5)\n",
        "filtered_df = filtered_df.reset_index(drop = True)\n",
        "filtered_df_2 = filtered_df\n",
        "filtered_df_2['Median'] = filtered_df_2['Median'] * 100\n",
        "filtered_df_2['Q1'] = filtered_df_2['Q1'] * 100\n",
        "filtered_df_2['Q3'] = filtered_df_2['Q3'] * 100"
      ],
      "metadata": {
        "id": "yCVRjF4d--QG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install forestplot\n",
        "import forestplot as fp"
      ],
      "metadata": {
        "id": "bR7RLWms_Nir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp.forestplot(filtered_df_2,  # the dataframe with results data\n",
        "              estimate='Median',  # col containing estimated effect size\n",
        "              ll='Q1', hl='Q3',  # columns containing conf. int. lower and higher limits\n",
        "              varlabel='label',  # column containing variable label\n",
        "              capitalize=\"capitalize\",  # Capitalize labels\n",
        "              groupvar=\"group\",  # Add variable groupings\n",
        "              #decimal_precision=4,\n",
        "              group_order=['Temporal & Movement', 'Gaze & Pupil', 'Head Movement & Orientation'],\n",
        "              #ylabel=\"Confidence interval\",  # y-label title\n",
        "              xlabel=\"Decrease in Accuracy Score % \",  # x-label title\n",
        "              **{#\"marker\": \"D\",  # set maker symbol as diamond\n",
        "                 #\"markersize\": 35,  # adjust marker size\n",
        "                 \"xlinestyle\": (0, (10, 5)),  # long dash for x-reference line\n",
        "                 \"xlinecolor\": \"#808080\",  # gray color for x-reference line\n",
        "                 \"xtick_size\": 12,  # adjust x-ticker fontsize\n",
        "                }\n",
        "              )"
      ],
      "metadata": {
        "id": "8cT7yn2M_P6E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}